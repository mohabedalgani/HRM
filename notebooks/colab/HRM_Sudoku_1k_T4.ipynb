{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTw8a8fKz36D"
      },
      "source": [
        "# üß© HRM Sudoku-Extreme 1 k Demo\n",
        "**Google Colab PRO (High-RAM) + T4 GPU ‚Äì single-GPU reproduction of the paper‚Äôs 1 k-shot run.**  \n",
        "Runtime: ~50 min on A100-high-ram, ~55 min on T4-high-ram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eF-0O0Bz36L",
        "outputId": "b47177e5-1253-41b9-b3a7-d4c717912aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug  2 12:45:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@title 0Ô∏è‚É£ Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJZYWmbGz36N"
      },
      "outputs": [],
      "source": [
        "#@title 1Ô∏è‚É£ One-liner installs (CUDA 12.6 + PyTorch 2.4 + Flash-Attn 2)\n",
        "import os, subprocess, sys\n",
        "def run(cmd):\n",
        "    try:\n",
        "        subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Command failed:\", e.cmd)\n",
        "        print(\"Return code:\", e.returncode)\n",
        "        print(\"Output (stdout):\", e.stdout)\n",
        "        print(\"Error (stderr):\", e.stderr)\n",
        "        raise # Re-raise the exception after printing\n",
        "\n",
        "# PyTorch 2.7 + CUDA 12.6 wheels\n",
        "run(\"pip install torch==2.7.0+cu126 --index-url https://download.pytorch.org/whl/cu126 --no-cache-dir --force-reinstall\")\n",
        "run(\"pip install torchvision==0.22.1+cu126 --index-url https://download.pytorch.org/whl/cu126 --no-cache-dir --force-reinstall\")\n",
        "run(\"pip install torchaudio==2.2.0+cu126 --index-url https://download.pytorch.org/whl/cu126 --no-cache-dir --force-reinstall\")\n",
        "\n",
        "\n",
        "# Ninja + setuptools for compilation\n",
        "run(\"pip install packaging ninja wheel setuptools setuptools-scm\")\n",
        "\n",
        "# Flash-Attention 2 (works on T4/A100)\n",
        "run(\"pip install flash-attn --no-build-isolation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaDXj4aDz36O"
      },
      "outputs": [],
      "source": [
        "#@title 2Ô∏è‚É£ Clone HRM repo + submodules\n",
        "run(\"git clone --recursive https://github.com/sapientinc/HRM.git\")\n",
        "%cd HRM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNT4J3ATz36P"
      },
      "outputs": [],
      "source": [
        "#@title 3Ô∏è‚É£ Python deps\n",
        "run(\"pip install -r requirements.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQxBUdNPz36Q"
      },
      "source": [
        "## 4Ô∏è‚É£ Build the Sudoku-Extreme 1 k dataset  \n",
        "This is exactly the same as the paper‚Äôs `subsample-size 1000 --num-aug 1000`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLNx0XfRz36Q"
      },
      "outputs": [],
      "source": [
        "#@title 4Ô∏è‚É£ Build dataset (~30 s)\n",
        "run(\"python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000 --subsample-size 1000 --num-aug 1000\")\n",
        "!ls data/sudoku-extreme-1k-aug-1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABj5pP_-z36R"
      },
      "source": [
        "## 5Ô∏è‚É£ Train (single GPU, small batch)\n",
        "We halve the batch size (192 instead of 384) to fit T4 16 GB.  \n",
        "The run will auto-log to Weights & Biases if you‚Äôre logged in (`wandb login`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gALlWurYz36R"
      },
      "outputs": [],
      "source": [
        "#@title 5Ô∏è‚É£ Launch training\n",
        "cmd = \"\"\"\n",
        "OMP_NUM_THREADS=8 python pretrain.py \\\n",
        "  data_path=data/sudoku-extreme-1k-aug-1000 \\\n",
        "  epochs=2000 \\\n",
        "  eval_interval=500 \\\n",
        "  global_batch_size=192 \\\n",
        "  lr=7e-5 \\\n",
        "  puzzle_emb_lr=7e-5 \\\n",
        "  weight_decay=1.0 \\\n",
        "  puzzle_emb_weight_decay=1.0 \\\n",
        "  wandb_project=\"hrm-colab-sudoku1k\"\n",
        "\"\"\"\n",
        "run(cmd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVx4GmjPz36S"
      },
      "source": [
        "## 6Ô∏è‚É£ Evaluate\n",
        "After training finishes (~step 1500) we run the built-in exact-accuracy evaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua4P29zbz36S"
      },
      "outputs": [],
      "source": [
        "#@title 6Ô∏è‚É£ Evaluate last checkpoint\n",
        "ckpt_path = !ls -t checkpoints/*/ckpt.pt | head -1\n",
        "ckpt_path = ckpt_path[0]\n",
        "print(\"Evaluating\", ckpt_path)\n",
        "run(f\"python evaluate.py checkpoint={ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgzl9l-0z36S"
      },
      "source": [
        "## 7Ô∏è‚É£ Show one solved grid\n",
        "We decode the first validation sample back to a human-readable Sudoku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A43PzIFmz36S"
      },
      "outputs": [],
      "source": [
        "#@title 7Ô∏è‚É£ Pretty print a solved puzzle\n",
        "from src.utils.sudoku import Sudoku\n",
        "import torch\n",
        "\n",
        "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "model = ckpt[\"model\"]\n",
        "model.eval()\n",
        "\n",
        "from src.data.sudoku_dataset import SudokuDataset\n",
        "ds = SudokuDataset(\"data/sudoku-extreme-1k-aug-1000\", split=\"val\")\n",
        "sample = ds[0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(sample[\"input_ids\"].unsqueeze(0).cuda())\n",
        "pred = logits.argmax(-1).cpu()\n",
        "\n",
        "print(\"Input puzzle:\\n\", Sudoku(sample[\"input_ids\"].view(9,9)).grid)\n",
        "print(\"Model solution:\\n\", Sudoku(pred.view(9,9)).grid)\n",
        "print(\"Target:\\n\", Sudoku(sample[\"target\"].view(9,9)).grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bQkSFMDz36T"
      },
      "source": [
        "## 8Ô∏è‚É£ Save checkpoint to Drive (optional)\n",
        "Mount your Drive and copy the 120 MB checkpoint so others can load it instantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6LfsR7Hz36T"
      },
      "outputs": [],
      "source": [
        "#@title 8Ô∏è‚É£ Mount Drive & save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/hrm_sudoku1k_t4\"\n",
        "run(f\"mkdir -p {save_dir}\")\n",
        "run(f\"cp -r checkpoints {save_dir}\")\n",
        "print(\"Checkpoint saved to\", save_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}